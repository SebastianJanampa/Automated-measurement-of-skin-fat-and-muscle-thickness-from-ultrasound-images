{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated measurement of skin, fat and muscle thickness from ultrasound images\n",
    "\n",
    "<strong>FILE:</strong> phase1.ipynb\n",
    "\n",
    "<strong>PROJECT PHASE:</strong> Phase 1\n",
    "\n",
    "<strong>PYTHON_VERSION:</strong> 3.8.3\n",
    "\n",
    "<strong>AUTHOR:</strong> <a href=\"https://www.linkedin.com/in/sebastianjr/\">Sebastian Janampa Rojas</a>\n",
    "\n",
    "<strong>EMAIL:</strong> sebastian.janampa@utec.edu.pe\n",
    "\n",
    "<strong>CREATE DATE:</strong> /03/2021 (DD/MM/YYYY)\n",
    "\n",
    "<strong>COMPLEMENTED FILES:</strong> utils.py\n",
    "\n",
    "<strong>AVAILABILITY OF DATA:</strong>:  <a href=\"https://multisbeta.stanford.edu/\">https://multisbeta.stanford.edu/</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<strong>ACKNOWLEDGEMENT</strong>\n",
    "\n",
    "This project is possible thanks to <strong>Erdemir Lab</strong> who provided the ultrasound images and did the manual annotation of the images. More information about their projec is available at \n",
    "\n",
    "---\n",
    "\n",
    "This Jupyter Notebook is divided in:\n",
    "1. Libraries\n",
    "1. Import Dataset\n",
    "1. Pre-processing\n",
    "1. Data Normalization\n",
    "1. Costum loss function\n",
    "1. Categorical data\n",
    "\n",
    "---\n",
    "\n",
    "## Libraries\n",
    "In the next cell, we will import the librarias used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import utils\n",
    "# # For reproducibility\n",
    "# np.random.seed(1)\n",
    "# tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Dataset</h2>\n",
    "\n",
    "_**Directory**_\n",
    "\n",
    "    Thesis:\n",
    "        ProjectCode:\n",
    "            Phase1:\n",
    "                utils.py\n",
    "                phase1.ipynb\n",
    "        NN:\n",
    "            data:\n",
    "                Multisxxx-1:\n",
    "                    Ultrasound_minFrame:\n",
    "                        IMA files\n",
    "                    categorical_values.csv\n",
    "                    thickness.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd() # Current directory\n",
    "main_dir = '\\\\'.join(cur_dir.split('\\\\')[:-2]) # Main directory\n",
    "dat_dir = os.path.join(main_dir,'NN\\\\data') # Data directory\n",
    "\n",
    "for _,folder,_ in os.walk(dat_dir):\n",
    "    if len(folder)==100:\n",
    "        subjects = folder\n",
    "print('Total number of subjects: %i'%len(subjects))\n",
    "\n",
    "#### Split data ####\n",
    "perc_train = 0.75 # train percentage\n",
    "subjects_train, subjects_test = train_test_split(subjects, \n",
    "                                                 train_size=perc_train,\n",
    "                                                 random_state=3)\n",
    "subjects_val,subjects_test=train_test_split(subjects_test, \n",
    "                                            train_size=15/25, \n",
    "                                            random_state=3)\n",
    "print('# of subjects used in training: %i'%len(subjects_train))\n",
    "print('# of subjects used in validation: %i'%len(subjects_val))\n",
    "print('# of subjects used in testing: %i'%len(subjects_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "datasets = {'training': subjects_train, 'validation': subjects_val, 'test':subjects_test}\n",
    "training, validation, testing = utils.load_data(dat_dir, **datasets)\n",
    "\n",
    "# Unpacking\n",
    "x_img_train, x_ctg_train, y_train = training\n",
    "x_img_val, x_ctg_val, y_val = validation\n",
    "x_img_test, x_ctg_test, y_test = testing\n",
    "del training, validation, testing, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "[training_outputs, validation_outputs, testing_outputs], params = utils.normalization_tech([y_train, y_val, y_test],['std', 'lin', 'dec'])\n",
    "del y_train, y_val, y_test\n",
    "\n",
    "# Packing in datasets\n",
    "training = {'images': x_img_train, 'categories': x_ctg_train, 'thickness': training_outputs}\n",
    "validation = {'images': x_img_val, 'categories': x_ctg_val, 'thickness': validation_outputs}\n",
    "testing = {'images': x_img_test, 'categories': x_ctg_test, 'thickness': testing_outputs}\n",
    "del training_outputs, validation_outputs, testing_outputs \n",
    "del x_img_train, x_img_val, x_img_test\n",
    "del x_ctg_train, x_ctg_val, x_ctg_test\n",
    "\n",
    "# Removing outliers\n",
    "training, validation, testing = utils.remove_outliers([training, validation, testing], params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "In this section, the objective is to determine if normalizing the data is beneficial for the model.\n",
    "\n",
    "First, four models with the same architecture are created.\n",
    "\n",
    "- non -> no normalization techinique was applied\n",
    "- std -> z_score normalization was applied\n",
    "- lin -> linear scaling was applied\n",
    "- dec -> decimal scaling was applied\n",
    "\n",
    "---\n",
    "\n",
    "The models used the _**Mean-Squared Error**_ as loss function. Moreover, the _**Adam**_ optimizer is used with a learning rate of $10^{-4}$ . In additon the _**number of epochs**_ and _**batchsize**_ are $100$ and $64$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Dims of the input data: %s' % ', '.join(map(str, training['images'].shape[1:])), '(height, width, channels)')\n",
    "# Creating models\n",
    "tf.random.set_seed(1234)\n",
    "models = utils.create_models(utils.ModelAB, input_shape=training['images'].shape[1:], dic_weights=None, methods=['non','std', 'lin', 'dec'])\n",
    "# \n",
    "# Training the models\n",
    "# tf.random.set_seed(1234)\n",
    "results = utils.myFit(models, num_epochs=2, verbose=0, training=training, validation=validation)\n",
    "del models\n",
    "\n",
    "# Showing results\n",
    "utils.show_results(results, parameters=params, training=training,validation=validation, testing=testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Run the next cell to visualize the architecture of the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results['non']['model'].summary() #Table\n",
    "# keras.utils.plot_model(results['non']['model'], show_shapes=True) #Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_modelsv1(results, params,'datanormalizationv1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_modelsv2(results, params, 'datanormalizationv2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costum Loss Function\n",
    "\n",
    "In this section, a costum loss function is implemented in the networks whose data is normalized.\n",
    "\n",
    "---\n",
    "\n",
    "The costum loss function is:\n",
    "\n",
    "$J(Y, \\hat{Y}) = W*\\sum_{i=0}^{m}\\frac{(Y - \\hat{Y})^2}{m}$\n",
    "\n",
    "where $y$ and $\\hat{y}$ are the real and the predicted values, respectively. The loss function is the _**Mean-Squeared Error**_ multplied by a **Weight Vector** ($W$). Finally, $m$ represents the number of samples. The $*$ is the dot product.\n",
    "\n",
    "---\n",
    "Dimensions of the variables\n",
    "\n",
    "$Y=\n",
    "\\begin{pmatrix}\n",
    "y_{1,1} & y_{1,2} & y_{1,3} \\\\\n",
    "y_{1,1} & y_{1,2} & y_{1,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots  \\\\\n",
    "y_{1,m} & y_{1,m} & y_{1,m}\n",
    "\\end{pmatrix}$     Each column represent a type of tissue.\n",
    "\n",
    "$W=\n",
    "\\begin{pmatrix}\n",
    "w_{1,1} & w_{1,2} & w_{1,3}\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dic_weights' variable\n",
    "dic_weigths = {\n",
    "    'non': None,\n",
    "    'std': params['std'],\n",
    "    'lin': params['maxi'] - params['mini'],\n",
    "    'dec': params['dec_vals']\n",
    "}\n",
    "\n",
    "# Creating models\n",
    "models = utils.create_models(utils.ModelAB, input_shape=training['images'].shape[1:], dic_weights=dic_weigths, methods=['non','std', 'lin', 'dec'])\n",
    "\n",
    "# Training the models\n",
    "tf.random.set_seed(1234)\n",
    "results = utils.myFit(models, num_epochs=2, verbose=0, training=training, validation=validation)\n",
    "del models\n",
    "\n",
    "# Showing results\n",
    "utils.show_results(results, parameters=params, training=training,validation=validation, testing=testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
